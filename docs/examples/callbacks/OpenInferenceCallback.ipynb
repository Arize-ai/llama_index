{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenInference Callback Handler\n",
    "\n",
    "[OpenInference](https://github.com/Arize-ai/open-inference-spec) is an open standard for capturing and storing AI model inferences. It enables production LLMapp servers to seamlessly integrate with LLM observability solutions such as [Arize](https://arize.com/) and [Phoenix](https://github.com/Arize-ai/phoenix).\n",
    "\n",
    "The `OpenInferenceCallbackHandler` saves data from retrieval-augmented generation (RAG) systems for downstream analysis and debugging. In particular, it saves the following data in columnar format:\n",
    "\n",
    "- query IDs\n",
    "- query text\n",
    "- query embeddings\n",
    "- scores (e.g., cosine similarity)\n",
    "- retrieved document IDs\n",
    "\n",
    "This tutorial demonstrates the callback handler's use for both in-notebook experimentation and lightweight production logging.\n",
    "\n",
    "⚠️ The `OpenInferenceCallbackHandler` is in beta and its APIs are subject to change.\n",
    "\n",
    "ℹ️ The callback maintainers are actively working to support additional RAG use-cases. If you find that your particular query engine or use-case is not supported, email the maintainers at phoenix-devs@arize.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies and Import Libraries\n",
    "\n",
    "Install notebook dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install html2text llama-index pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import textwrap\n",
    "from typing import List, Union\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleWebPageReader,\n",
    "    ServiceContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.callbacks import CallbackManager, OpenInferenceCallbackHandler\n",
    "from llama_index.callbacks.open_inference_callback import as_dataframe, QueryData\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Parse Documents\n",
    "\n",
    "Load documents from Paul Graham's essay \"What I Worked On\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleWebPageReader().load_data(\n",
    "    [\n",
    "        \"http://raw.githubusercontent.com/jerryjliu/llama_index/main/examples/paul_graham_essay/data/paul_graham_essay.txt\"\n",
    "    ]\n",
    ")\n",
    "print(documents[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the document into nodes. Display the first node's text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "print(nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access RAG Data as a Pandas Dataframe\n",
    "\n",
    "When experimenting with chatbots and LLMapps in a notebook, it's often useful to run your chatbot against a small collection of user queries and collect and analyze the data for iterative improvement. The `OpenInferenceCallbackHandler` stores RAG data in columnar format and provides convenient access to the data as a pandas dataframe.\n",
    "\n",
    "Instantiate the OpenInference callback handler and attach to the service context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_handler = OpenInferenceCallbackHandler()\n",
    "callback_manager = CallbackManager([callback_handler])\n",
    "service_context = ServiceContext.from_defaults(callback_manager=callback_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the index and instantiate the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your query engine across a collection of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_characters_per_line = 80\n",
    "queries = [\n",
    "    \"What did Paul Graham do growing up?\",\n",
    "    \"When and how did Paul Graham's mother die?\",\n",
    "    \"What, in Paul Graham's opinion, is the most distinctive thing about YC?\",\n",
    "    \"When and how did Paul Graham meet Jessica Livingston?\",\n",
    "    \"What is Bel, and when and where was it written?\",\n",
    "]\n",
    "for query in queries:\n",
    "    response = query_engine.query(query)\n",
    "    print(\"Query\")\n",
    "    print(\"=====\")\n",
    "    print(textwrap.fill(query, max_characters_per_line))\n",
    "    print()\n",
    "    print(\"Response\")\n",
    "    print(\"========\")\n",
    "    print(textwrap.fill(str(response), max_characters_per_line))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from your query engine runs can be accessed as a pandas dataframe for analysis and iterative improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data_buffer = callback_handler.flush_query_data_buffer()\n",
    "query_dataframe = as_dataframe(query_data_buffer)\n",
    "query_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe column names conform to the OpenInference spec, which specifies the category, data type, and intent of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Production RAG Data\n",
    "\n",
    "In a production setting, LlamaIndex application maintainers can log the data generated by their system by implementing and passing a custom `callback` to `OpenInferenceCallbackHandler`. The callback is of type `Callable[List[QueryData]]` that accepts a buffer of query data from the `OpenInferenceCallbackHandler`, persists the data (e.g., by uploading to cloud storage or sending to a data ingestion service), and flushes the buffer after data is persisted. A reference implementation is included below that periodically writes data in OpenInference format to local Parquet files when the buffer exceeds a certain size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetCallback:\n",
    "    def __init__(self, data_path: Union[str, Path], max_buffer_length: int = 1000):\n",
    "        self._data_path = Path(data_path)\n",
    "        self._data_path.mkdir(parents=True, exist_ok=False)\n",
    "        self._max_buffer_length = max_buffer_length\n",
    "        self._batch_index = 0\n",
    "\n",
    "    def __call__(self, query_data_buffer: List[QueryData]) -> None:\n",
    "        if len(query_data_buffer) > self._max_buffer_length:\n",
    "            query_dataframe = as_dataframe(query_data_buffer)\n",
    "            file_path = self._data_path / f\"log-{self._batch_index}.parquet\"\n",
    "            query_dataframe.to_parquet(file_path)\n",
    "            self._batch_index += 1\n",
    "            query_data_buffer.clear()  # ⚠️ clear the buffer or it will keep growing forever!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ In a production setting, it's important to clear the buffer, otherwise, the callback handler will indefinitely accumulate data in memory and eventually cause your system to crash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach the Parquet writer to your callback and re-run the query engine. The RAG data will be saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"rag_data\"\n",
    "parquet_writer = ParquetCallback(\n",
    "    data_path=data_path,\n",
    "    # this parameter is set artificially low for demonstration purposes\n",
    "    # to force a flush to disk, in practice it would be much larger\n",
    "    max_buffer_length=1,\n",
    ")\n",
    "callback_handler = OpenInferenceCallbackHandler(callback=parquet_writer)\n",
    "callback_manager = CallbackManager([callback_handler])\n",
    "service_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    query_engine.query(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and display saved Parquet data from disk to verify that the logger is working. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataframes = []\n",
    "for file_name in os.listdir(data_path):\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    query_dataframes.append(pd.read_parquet(file_path))\n",
    "query_dataframe = pd.concat(query_dataframes)\n",
    "query_dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenixdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
